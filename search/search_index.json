{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>A engenharia de software \u00e9 um campo da computa\u00e7\u00e3o que se dedica ao estudo, desenvolvimento e manuten\u00e7\u00e3o de sistemas de software de maneira estruturada e eficiente. Seu objetivo \u00e9 aplicar princ\u00edpios da engenharia para garantir que os sistemas sejam confi\u00e1veis, escal\u00e1veis, seguros e f\u00e1ceis de manter.</p>"},{"location":"#principios-da-engenharia-de-software","title":"Princ\u00edpios da Engenharia de Software","text":"<p>Os princ\u00edpios fundamentais da engenharia de software incluem modularidade, reutiliza\u00e7\u00e3o de c\u00f3digo, manuten\u00e7\u00e3o cont\u00ednua e boas pr\u00e1ticas de desenvolvimento. Esses princ\u00edpios permitem a cria\u00e7\u00e3o de sistemas robustos e adapt\u00e1veis \u00e0s mudan\u00e7as tecnol\u00f3gicas e aos requisitos do usu\u00e1rio.</p>"},{"location":"#ciclo-de-vida-do-software","title":"Ciclo de Vida do Software","text":"<p>O desenvolvimento de software segue um ciclo de vida, que pode incluir as seguintes etapas:</p> <ol> <li>Levantamento de Requisitos - Defini\u00e7\u00e3o clara dos objetivos e funcionalidades do software.</li> <li>Projeto (Design) - Elabora\u00e7\u00e3o da arquitetura do sistema, defini\u00e7\u00e3o de tecnologias e escolha de padr\u00f5es de projeto.</li> <li>Implementa\u00e7\u00e3o - Desenvolvimento do software utilizando linguagens de programa\u00e7\u00e3o adequadas.</li> <li>Testes - Avalia\u00e7\u00e3o do software para garantir sua qualidade e funcionamento correto.</li> <li>Implanta\u00e7\u00e3o - Distribui\u00e7\u00e3o do software para uso real.</li> <li>Manuten\u00e7\u00e3o e Atualiza\u00e7\u00e3o - Corre\u00e7\u00e3o de erros, melhorias e adi\u00e7\u00e3o de novas funcionalidades.</li> </ol>"},{"location":"#metodologias-de-desenvolvimento","title":"Metodologias de Desenvolvimento","text":"<p>Existem diversas metodologias para guiar o processo de desenvolvimento, entre as mais utilizadas est\u00e3o:</p> <ul> <li>Modelo Cascata: Segue uma sequ\u00eancia linear de etapas, ideal para projetos com requisitos bem definidos.</li> <li>Metodologias \u00c1geis: Como Scrum e Kanban, promovem desenvolvimento iterativo, colabora\u00e7\u00e3o entre equipes e entrega cont\u00ednua de software.</li> <li>DevOps: Integra\u00e7\u00e3o entre desenvolvimento e opera\u00e7\u00e3o para automa\u00e7\u00e3o e melhoria cont\u00ednua do software.</li> </ul>"},{"location":"#desafios-na-engenharia-de-software","title":"Desafios na Engenharia de Software","text":"<p>Os engenheiros de software enfrentam desafios como:</p> <ul> <li>Gerenciamento de complexidade em grandes sistemas.</li> <li>Manuten\u00e7\u00e3o e atualiza\u00e7\u00e3o de c\u00f3digo legado.</li> <li>Garantia de seguran\u00e7a contra amea\u00e7as cibern\u00e9ticas.</li> <li>Adapta\u00e7\u00e3o a novas tecnologias e paradigmas de programa\u00e7\u00e3o.</li> </ul> <p>A engenharia de software \u00e9 uma \u00e1rea essencial para a inova\u00e7\u00e3o e evolu\u00e7\u00e3o da tecnologia, permitindo a cria\u00e7\u00e3o de solu\u00e7\u00f5es eficientes e confi\u00e1veis para os desafios do mundo moderno.</p>"},{"location":"data_pipelines/","title":"Pipelines de Dados","text":"<p>Pipelines de dados s\u00e3o a base para o sucesso em an\u00e1lises de dados e aprendizado de m\u00e1quina. Mover dados de diversas fontes variadas e process\u00e1-los para fornecer contexto \u00e9 a diferen\u00e7a entre ter dados e obter valor deles.</p>"},{"location":"data_pipelines/pocket_reference/01-introduction-to-data-pipelines/","title":"Introdu\u00e7\u00e3o a Pipelines de Dados","text":"<p>Por tr\u00e1s de cada dashboard brilhante, modelo de aprendizado de m\u00e1quina e insight que muda neg\u00f3cios, est\u00e1 o dado. N\u00e3o apenas dados brutos, mas dados coletados de in\u00fameras fontes que precisam ser limpos, processados e combinados para entregar valor. A famosa frase \"dados s\u00e3o o novo petr\u00f3leo\" provou ser verdadeira. Assim como o petr\u00f3leo, o valor dos dados est\u00e1 em seu potencial ap\u00f3s serem refinados e entregues ao consumidor. Tamb\u00e9m como o petr\u00f3leo, s\u00e3o necess\u00e1rios pipelines eficientes para entregar dados atrav\u00e9s de cada est\u00e1gio de sua cadeia de valor.</p>"},{"location":"data_pipelines/pocket_reference/01-introduction-to-data-pipelines/#o-que-sao-pipelines-de-dados","title":"O Que S\u00e3o Pipelines de Dados?","text":"<p>Pipelines de dados s\u00e3o conjuntos de processos que movem e transformam dados de v\u00e1rias fontes para um destino onde novo valor pode ser derivado. Eles s\u00e3o a base das capacidades de an\u00e1lise, relat\u00f3rios e aprendizado de m\u00e1quina.</p> <p>A complexidade de um pipeline de dados depende do tamanho, estado e estrutura dos dados de origem, bem como das necessidades do projeto de an\u00e1lise. Na sua forma mais simples, pipelines podem extrair dados de uma \u00fanica fonte, como uma API REST, e carregar para um destino, como uma tabela SQL em um data warehouse. Na pr\u00e1tica, no entanto, pipelines geralmente consistem em m\u00faltiplas etapas, incluindo extra\u00e7\u00e3o de dados, pr\u00e9-processamento de dados, valida\u00e7\u00e3o de dados e entrega dos dados ao seu destino final. Pipelines frequentemente cont\u00eam tarefas de m\u00faltiplos sistemas e linguagens de programa\u00e7\u00e3o. Al\u00e9m disso, equipes de dados geralmente possuem e mant\u00eam v\u00e1rios pipelines de dados que compartilham depend\u00eancias e precisam ser coordenados.</p>"},{"location":"data_pipelines/pocket_reference/01-introduction-to-data-pipelines/#quem-constroi-pipelines-de-dados","title":"Quem Constr\u00f3i Pipelines de Dados?","text":"<p>Com a populariza\u00e7\u00e3o da computa\u00e7\u00e3o em nuvem e do software como servi\u00e7o (SaaS), o n\u00famero de fontes de dados que as organiza\u00e7\u00f5es precisam entender explodiu. Ao mesmo tempo, a demanda por dados para alimentar modelos de aprendizado de m\u00e1quina, pesquisas de ci\u00eancia de dados e insights sens\u00edveis ao tempo \u00e9 maior do que nunca. Para acompanhar, a engenharia de dados emergiu como um papel fundamental nas equipes de an\u00e1lise. Engenheiros de dados se especializam em construir e manter os pipelines de dados que sustentam o ecossistema de an\u00e1lise.</p> <p>O objetivo de um engenheiro de dados n\u00e3o \u00e9 simplesmente carregar dados em um data warehouse. Engenheiros de dados trabalham de perto com cientistas de dados e analistas para entender o que ser\u00e1 feito com os dados e ajudar a trazer suas necessidades para um estado de produ\u00e7\u00e3o escal\u00e1vel.</p> <p>Engenheiros de dados se orgulham de garantir a validade e a pontualidade dos dados que entregam. Isso significa testar, alertar e criar planos de conting\u00eancia para quando algo der errado. E sim, algo eventualmente dar\u00e1 errado!</p> <p>As habilidades espec\u00edficas de um engenheiro de dados dependem um pouco da pilha de tecnologia que sua organiza\u00e7\u00e3o usa. No entanto, h\u00e1 algumas habilidades comuns que todos os bons engenheiros de dados possuem.</p>"},{"location":"data_pipelines/pocket_reference/01-introduction-to-data-pipelines/#fundamentos-de-sql-e-data-warehousing","title":"Fundamentos de SQL e Data Warehousing","text":"<p>Engenheiros de dados precisam saber como consultar bancos de dados, e SQL \u00e9 a linguagem universal para fazer isso. Engenheiros de dados experientes sabem como escrever SQL de alto desempenho e entendem os fundamentos de data warehousing e modelagem de dados. Mesmo que uma equipe de dados inclua especialistas em data warehousing, um engenheiro de dados com fundamentos de warehousing \u00e9 um parceiro melhor e pode preencher lacunas t\u00e9cnicas mais complexas que surgem.</p>"},{"location":"data_pipelines/pocket_reference/01-introduction-to-data-pipelines/#python-eou-java","title":"Python e/ou Java","text":"<p>A linguagem na qual um engenheiro de dados \u00e9 proficiente depender\u00e1 da pilha de tecnologia de sua equipe, mas de qualquer forma, um engenheiro de dados n\u00e3o conseguir\u00e1 fazer o trabalho com ferramentas \"sem c\u00f3digo\", mesmo que tenha algumas boas em seu arsenal. Python e Java atualmente dominam na engenharia de dados, mas novos como Go est\u00e3o surgindo.</p>"},{"location":"data_pipelines/pocket_reference/01-introduction-to-data-pipelines/#computacao-distribuida","title":"Computa\u00e7\u00e3o Distribu\u00edda","text":"<p>Resolver um problema que envolve grande volume de dados e o desejo de processar dados rapidamente levou os engenheiros de dados a trabalhar com plataformas de computa\u00e7\u00e3o distribu\u00edda. A computa\u00e7\u00e3o distribu\u00edda combina o poder de m\u00faltiplos sistemas para armazenar, processar e analisar grandes volumes de dados de forma eficiente.</p> <p>Um exemplo popular de computa\u00e7\u00e3o distribu\u00edda em an\u00e1lises \u00e9 o ecossistema Hadoop, que inclui armazenamento de arquivos distribu\u00eddos via Hadoop Distributed File System (HDFS), processamento via MapReduce, an\u00e1lise de dados via Pig, e mais. O Apache Spark \u00e9 outro framework de processamento distribu\u00eddo popular, que est\u00e1 rapidamente superando o Hadoop em popularidade.</p> <p>Embora nem todos os pipelines de dados exijam o uso de computa\u00e7\u00e3o distribu\u00edda, os engenheiros de dados precisam saber como e quando utilizar tal framework.</p>"},{"location":"data_pipelines/pocket_reference/01-introduction-to-data-pipelines/#administracao-basica-de-sistemas","title":"Administra\u00e7\u00e3o B\u00e1sica de Sistemas","text":"<p>Espera-se que um engenheiro de dados seja proficiente na linha de comando do Linux e seja capaz de realizar tarefas como analisar logs de aplicativos, agendar cron jobs e solucionar problemas de firewall e outras configura\u00e7\u00f5es de seguran\u00e7a. Mesmo trabalhando totalmente em um provedor de nuvem como AWS, Azure ou Google Cloud, eles acabar\u00e3o usando essas habilidades para fazer os servi\u00e7os em nuvem funcionarem juntos e implantar pipelines de dados.</p>"},{"location":"data_pipelines/pocket_reference/01-introduction-to-data-pipelines/#uma-mentalidade-orientada-a-objetivos","title":"Uma Mentalidade Orientada a Objetivos","text":"<p>Um bom engenheiro de dados n\u00e3o possui apenas habilidades t\u00e9cnicas. Eles podem n\u00e3o interagir com os stakeholders regularmente, mas os analistas e cientistas de dados da equipe certamente o far\u00e3o. O  engenheiro de dados tomar\u00e1 melhores decis\u00f5es arquitet\u00f4nicas se estiver ciente do motivo pelo qual  est\u00e1 construindo um pipeline em primeiro lugar.</p>"},{"location":"data_pipelines/pocket_reference/01-introduction-to-data-pipelines/#por-que-construir-pipelines-de-dados","title":"Por Que Construir Pipelines de Dados?","text":"<p>Da mesma forma que a ponta do iceberg \u00e9 tudo o que pode ser visto por um navio que passa, o produto final do fluxo de trabalho de an\u00e1lise \u00e9 tudo o que a maioria de uma organiza\u00e7\u00e3o v\u00ea. Executivos veem dashboards e gr\u00e1ficos impec\u00e1veis. O marketing compartilha insights bem embalados nas redes sociais. O suporte ao cliente otimiza o atendimento do call center com base no resultado de um modelo preditivo de demanda.</p> <p>O que a maioria das pessoas fora da \u00e1rea de an\u00e1lise muitas vezes n\u00e3o consegue apreciar \u00e9 que, para gerar o que \u00e9 visto, h\u00e1 uma maquinaria complexa que \u00e9 invis\u00edvel. Para cada dashboard e insight que um analista de dados gera e para cada modelo preditivo desenvolvido por um cientista de dados, h\u00e1 pipelines de dados trabalhando nos bastidores. N\u00e3o \u00e9 incomum que um \u00fanico dashboard, ou mesmo uma \u00fanica m\u00e9trica, seja derivado de dados originados em m\u00faltiplos sistemas de origem. Al\u00e9m disso, pipelines de dados fazem mais do que apenas extrair dados de fontes e carreg\u00e1-los em tabelas de banco de dados simples ou arquivos planos para os analistas usarem. Os dados brutos s\u00e3o refinados ao longo do caminho para serem limpos, estruturados, normalizados, combinados, agregados e, \u00e0s vezes, anonimizados ou de outra forma protegidos. Em outras palavras, h\u00e1 muito mais acontecendo abaixo da linha d'\u00e1gua.</p>"},{"location":"data_pipelines/pocket_reference/01-introduction-to-data-pipelines/#como-os-pipelines-sao-construidos","title":"Como os Pipelines S\u00e3o Constru\u00eddos?","text":"<p>Junto com os engenheiros de dados, in\u00fameras ferramentas para construir e dar suporte a pipelines de dados surgiram nos \u00faltimos anos. Algumas s\u00e3o open source, outras comerciais, e algumas s\u00e3o desenvolvidas internamente. Alguns pipelines s\u00e3o escritos em Python, outros em Java, alguns em outra linguagem, e alguns sem c\u00f3digo algum.</p> <p>Al\u00e9m disso, pipelines n\u00e3o s\u00e3o apenas constru\u00eddos - eles s\u00e3o monitorados, mantidos e estendidos. Os engenheiros de dados s\u00e3o respons\u00e1veis n\u00e3o apenas por entregar dados uma vez, mas por construir pipelines e infraestrutura de suporte que entreguem e processem dados de forma confi\u00e1vel, segura e pontual. N\u00e3o \u00e9 uma tarefa f\u00e1cil, mas quando \u00e9 bem feita, o valor dos dados de uma organiza\u00e7\u00e3o pode realmente ser desbloqueado.</p>"},{"location":"data_pipelines/pocket_reference/02-a-modern-data-infrastructure/","title":"Uma Infraestrutura de Dados Moderna","text":"<p>Antes de decidir sobre produtos e design para construir pipelines, vale a pena entender o que comp\u00f5e uma pilha de dados moderna. Como na maioria das coisas em tecnologia, n\u00e3o h\u00e1 uma \u00fanica maneira correta de projetar seu ecossistema de an\u00e1lise ou escolher produtos e fornecedores. Independentemente disso, h\u00e1 algumas necessidades e conceitos chave que se tornaram padr\u00e3o na ind\u00fastria e estabelecem o cen\u00e1rio para as melhores pr\u00e1ticas na implementa\u00e7\u00e3o de pipelines.</p>"},{"location":"data_pipelines/pocket_reference/02-a-modern-data-infrastructure/#diversidade-de-fontes-de-dados","title":"Diversidade de Fontes de Dados","text":"<p>A maioria das organiza\u00e7\u00f5es possui dezenas, se n\u00e3o centenas, de fontes de dados que alimentam seus esfor\u00e7os anal\u00edticos. As fontes de dados variam em muitas dimens\u00f5es abordadas nesta se\u00e7\u00e3o.</p> <p>Os principais componentes de uma infraestrutura de dados moderna</p> <ul> <li>Data warehouses na nuvem e data lakes</li> <li>Ferramentas de ingest\u00e3o de dados</li> <li>Diversidade de fontes de dados</li> <li>Plataformas de orquestra\u00e7\u00e3o de workflows</li> <li>Ferramentas e frameworks de modelagem</li> </ul>"},{"location":"data_pipelines/pocket_reference/02-a-modern-data-infrastructure/#propriedade-do-sistema-de-origem","title":"Propriedade do Sistema de Origem","text":"<p>\u00c9 comum para uma equipe de an\u00e1lise ingerir dados de sistemas de origem que s\u00e3o constru\u00eddos e mantidos pela organiza\u00e7\u00e3o, bem como de ferramentas e fornecedores de terceiros. Por exemplo, uma empresa de e-commerce pode armazenar dados de seu carrinho de compras em um banco de dados PostgreSQL (tamb\u00e9m conhecido como Postgres) por tr\u00e1s de seu aplicativo web. Eles tamb\u00e9m podem usar uma ferramenta de an\u00e1lise web de terceiros, como o Google Analytics, para rastrear o uso em seu site. A combina\u00e7\u00e3o dessas duas fontes de dados \u00e9 necess\u00e1ria para obter uma compreens\u00e3o completa do comportamento do cliente at\u00e9 a compra. Assim, um pipeline de dados que termina com uma an\u00e1lise de tal comportamento come\u00e7a com a ingest\u00e3o de dados de ambas as fontes.</p> <p>Entender a propriedade dos sistemas de origem \u00e9 importante por v\u00e1rias raz\u00f5es. Primeiro, para fontes de dados de terceiros, voc\u00ea provavelmente estar\u00e1 limitado quanto aos dados que pode acessar e como pode acess\u00e1-los. A maioria dos fornecedores disponibiliza uma API REST, mas poucos lhe dar\u00e3o acesso direto aos seus dados na forma de um banco de dados SQL. Ainda menos fornecer\u00e3o muita personaliza\u00e7\u00e3o sobre quais dados voc\u00ea pode acessar e em que n\u00edvel de granularidade.</p> <p>Sistemas constru\u00eddos internamente apresentam \u00e0 equipe de an\u00e1lise mais oportunidades de personalizar os dados dispon\u00edveis, bem como o m\u00e9todo de acesso. No entanto, eles tamb\u00e9m apresentam outros desafios. Os sistemas foram constru\u00eddos com considera\u00e7\u00e3o \u00e0 ingest\u00e3o de dados? Muitas vezes a resposta \u00e9 n\u00e3o, o que tem implica\u00e7\u00f5es que variam desde a ingest\u00e3o colocando uma carga n\u00e3o intencional no sistema at\u00e9 a incapacidade de carregar dados incrementalmente. Se voc\u00ea tiver sorte, a equipe de engenharia que possui o sistema de origem ter\u00e1 tempo e disposi\u00e7\u00e3o para trabalhar com voc\u00ea, mas na realidade das restri\u00e7\u00f5es de recursos, voc\u00ea pode descobrir que n\u00e3o \u00e9 muito diferente de trabalhar com um fornecedor externo.</p>"},{"location":"data_pipelines/pocket_reference/02-a-modern-data-infrastructure/#interface-de-ingestao-e-estrutura-de-dados","title":"Interface de Ingest\u00e3o e Estrutura de Dados","text":"<p>Independentemente de quem possui os dados de origem, como voc\u00ea os obt\u00e9m e em que forma \u00e9 a primeira coisa que um engenheiro de dados examinar\u00e1 ao construir uma nova ingest\u00e3o de dados. Primeiro, qual \u00e9 a interface para os dados? Alguns dos mais comuns incluem os seguintes:</p> <ul> <li>Um banco de dados por tr\u00e1s de um aplicativo, como um banco de dados Postgres ou MySQL</li> <li>Uma camada de abstra\u00e7\u00e3o em cima de um sistema, como uma API REST</li> <li>Uma plataforma de processamento de streams, como o Apache Kafka</li> <li>Um sistema de arquivos de rede compartilhado ou bucket de armazenamento em nuvem contendo logs, arquivos CSV (valores separados por v\u00edrgula) e outros arquivos planos (flat files)</li> <li>Um data warehouse ou data lake</li> <li>Dados em HDFS ou banco de dados HBase</li> </ul> <p>Al\u00e9m da interface, a estrutura dos dados variar\u00e1. Aqui est\u00e3o alguns exemplos comuns:</p> <ul> <li>JSON de uma API REST</li> <li>Dados bem estruturados de um banco de dados MySQL</li> <li>JSON dentro de colunas de uma tabela de banco de dados MySQL</li> <li>Dados de log semiestruturados</li> <li>CSV, formato de largura fixa (FWF) e outros formatos de arquivos planos</li> <li>JSON em arquivos planos</li> <li>Sa\u00edda de stream do Kafka</li> </ul> <p>Cada interface e estrutura de dados apresenta seus pr\u00f3prios desafios e oportunidades. Dados bem estruturados s\u00e3o frequentemente mais f\u00e1ceis de trabalhar, mas geralmente s\u00e3o estruturados no interesse de um aplicativo ou site. Al\u00e9m da ingest\u00e3o dos dados, etapas adicionais no pipeline provavelmente ser\u00e3o necess\u00e1rias para limpar e transformar em uma estrutura em que os dados sejam reduzidos.</p> <p>Dados n\u00e3o estruturados s\u00e3o comuns para alguns empreendimentos anal\u00edticos. Por exemplo, modelos de Processamento de Linguagem Natural (Natural Language Processing - NLP) requerem grandes quantidades de dados de texto livre para treinar e validar. Projetos de Vis\u00e3o Computacional (Computer Vision - CV) requerem imagens e conte\u00fado de v\u00eddeo. Mesmo projetos menos assustadores, como a extra\u00e7\u00e3o de dados de p\u00e1ginas da web, t\u00eam necessidade de dados de texto livre da web, al\u00e9m da marca\u00e7\u00e3o HTML semiestruturada de uma p\u00e1gina da web.</p>"},{"location":"data_pipelines/pocket_reference/02-a-modern-data-infrastructure/#volume-de-dados","title":"Volume de Dados","text":"<p>Embora engenheiros de dados e gerentes de contrata\u00e7\u00e3o gostem de se gabar sobre conjuntos de dados em escala de petabytes, a realidade \u00e9 que a maioria das organiza\u00e7\u00f5es valoriza conjuntos de dados pequenos tanto quanto os grandes. Al\u00e9m disso, \u00e9 comum ingerir e modelar pequenos e grandes conjuntos de dados em conjunto. Embora as decis\u00f5es de design em cada etapa de um pipeline devam levar o volume de dados em considera\u00e7\u00e3o, alto volume n\u00e3o significa alto valor.</p> <p>Dito isso, a maioria das organiza\u00e7\u00f5es tem pelo menos um conjunto de dados que \u00e9 crucial tanto para as necessidades anal\u00edticas quanto para o alto volume. O que \u00e9 alto volume? N\u00e3o h\u00e1 uma defini\u00e7\u00e3o f\u00e1cil, mas no que diz respeito a pipelines, \u00e9 melhor pensar em termos de um espectro, em vez de uma defini\u00e7\u00e3o bin\u00e1ria de conjuntos de dados de alto e baixo volume.</p>"},{"location":"data_pipelines/pocket_reference/02-a-modern-data-infrastructure/#limpeza-e-validade-dos-dados","title":"Limpeza e Validade dos Dados","text":"<p>Assim como h\u00e1 grande diversidade nas fontes de dados, a qualidade dos dados de origem varia muito. Como diz o velho ditado, \"lixo entra, lixo sai\". \u00c9 importante entender as limita\u00e7\u00f5es e defici\u00eancias dos dados de origem e abord\u00e1-las nas se\u00e7\u00f5es apropriadas do seu pipeline.</p> <p>Existem muitas caracter\u00edsticas comuns de \"dados bagun\u00e7ados\", incluindo, mas n\u00e3o limitadas a, as seguintes:</p> <ul> <li>Registros duplicados ou amb\u00edguos</li> <li>Registros \u00f3rf\u00e3os</li> <li>Registros incompletos ou ausentes</li> <li>Erros de codifica\u00e7\u00e3o de texto</li> <li>Formatos inconsistentes (por exemplo, n\u00fameros de telefone com ou sem tra\u00e7os)</li> <li>Dados rotulados incorretamente ou sem r\u00f3tulo</li> </ul> <p>Claro, existem in\u00fameras outras, bem como quest\u00f5es de validade dos dados espec\u00edficas ao contexto do sistema de origem.</p>"},{"location":"data_pipelines/pocket_reference/02-a-modern-data-infrastructure/#latencia-e-largura-de-banda-do-sistema-de-origem","title":"Lat\u00eancia e Largura de Banda do Sistema de Origem","text":"<p>A necessidade de extrair frequentemente grandes volumes de dados dos sistemas de origem \u00e9 um caso de uso comum em uma pilha de dados moderna. No entanto, isso apresenta desafios. As etapas de extra\u00e7\u00e3o de dados nos pipelines devem lidar com limites de taxa de API, tempos de conex\u00e3o esgotados, downloads lentos e propriet\u00e1rios de sistemas de origem que ficam insatisfeitos devido \u00e0 carga colocada em seus sistemas.</p>"},{"location":"data_pipelines/pocket_reference/02-a-modern-data-infrastructure/#data-warehouses-na-nuvem-e-data-lakes","title":"Data Warehouses na Nuvem e Data Lakes","text":"<p>Tr\u00eas coisas transformaram o cen\u00e1rio da an\u00e1lise e do armazenamento de dados nos \u00faltimos 10 anos, e todas est\u00e3o relacionadas ao surgimento dos principais provedores de nuvem p\u00fablica (Amazon, Google e Microsoft).</p> <ul> <li>A facilidade de construir e implantar pipelines de dados, data lakes, data warehouses e processamento anal\u00edtico na nuvem. N\u00e3o h\u00e1 mais necessidade de esperar pelos departamentos de TI e pela aprova\u00e7\u00e3o do or\u00e7amento para grandes custos iniciais. Servi\u00e7os gerenciados -- bancos de dados em particular -- se tornaram comuns.</li> <li>A cont\u00ednua queda nos custos de armazenamento na nuvem.</li> <li>O surgimento de bancos de dados colunar altamente escal\u00e1veis, como Amazon Redshift, Snowflake e Google Big Query.</li> </ul> <p>Um data warehouse \u00e9 onde os dados de diferentes sistemas s\u00e3o armazenados e modelados para suportar an\u00e1lises e outras atividades relacionadas a responder perguntas com esses dados. Os dados em um armaz\u00e9m de dados s\u00e3o estruturados e otimizados para consultas de relat\u00f3rios e an\u00e1lises.</p> <p>Um data lake \u00e9 onde os dados s\u00e3o armazenados, mas sem a estrutura ou otimiza\u00e7\u00e3o de consulta de um armaz\u00e9m de dados. Ele provavelmente conter\u00e1 um grande volume de dados, bem como uma variedade de tipos de dados. Por exemplo, um \u00fanico data lake pode conter uma cole\u00e7\u00e3o de postagens de blog armazenadas como arquivos de texto, arquivos planos extra\u00eddos de um banco de dados relacional e objetos JSON contendo eventos gerados por sensores em um sistema industrial, embora n\u00e3o seja otimizado para consultar esses dados com o objetivo de relat\u00f3rios e an\u00e1lises.</p> <p>H\u00e1 um lugar para ambos, data warehouses e data lakes, no mesmo ecossistema de dados, e os pipelines de dados frequentemente movem dados entre ambos.</p>"},{"location":"data_pipelines/pocket_reference/02-a-modern-data-infrastructure/#ferramentas-de-transformacao-e-modelagem-de-dados","title":"Ferramentas de Transforma\u00e7\u00e3o e Modelagem de Dados","text":"<p>Os termos modelagem de dados e transforma\u00e7\u00e3o de dados s\u00e3o frequentemente usados de forma intercambi\u00e1vel.</p> <ul> <li> <p>Transforma\u00e7\u00e3o de Dados: Transformar dados \u00e9 um termo amplo que \u00e9 representado pelo T em um processo ETL ou ELT. Uma transforma\u00e7\u00e3o pode ser algo t\u00e3o simples quanto converter um timestamp armazenado em uma tabela de um fuso hor\u00e1rio para outro. Tamb\u00e9m pode ser uma opera\u00e7\u00e3o mais complexa que cria uma nova m\u00e9trica a partir de m\u00faltiplas colunas de origem que s\u00e3o agregadas e filtradas atrav\u00e9s de alguma l\u00f3gica de neg\u00f3cios.</p> </li> <li> <p>Modelagem de Dados: A modelagem de dados \u00e9 um tipo mais espec\u00edfico de transforma\u00e7\u00e3o de dados. Um modelo de dados estrutura e define dados em um formato que \u00e9 entendido e otimizado para an\u00e1lise de dados. Um modelo de dados \u00e9 geralmente representado como uma ou mais tabelas em um armaz\u00e9m de dados.</p> </li> </ul> <p>Assim como a ingest\u00e3o de dados, existem v\u00e1rias metodologias e ferramentas presentes em uma infraestrutura de dados moderna. Como mencionado anteriormente, algumas ferramentas de ingest\u00e3o de dados fornecem algum n\u00edvel de capacidades de transforma\u00e7\u00e3o de dados, mas estas s\u00e3o frequentemente bastante simples. Por exemplo, para proteger informa\u00e7\u00f5es pessoalmente identific\u00e1veis (PII), pode ser desej\u00e1vel transformar um endere\u00e7o de e-mail em um valor hash que \u00e9 armazenado no destino final. Tal transforma\u00e7\u00e3o \u00e9 geralmente realizada durante o processo de ingest\u00e3o.</p> <p>Para transforma\u00e7\u00f5es de dados mais complexas e modelagem de dados, \u00e9 desej\u00e1vel buscar ferramentas e frameworks especificamente projetados para a tarefa, como o dbt. Al\u00e9m disso, a transforma\u00e7\u00e3o de dados \u00e9 frequentemente espec\u00edfica ao contexto e pode ser escrita em uma linguagem familiar para engenheiros de dados e analistas de dados, como SQL ou Python.</p> <p>Modelos de dados que ser\u00e3o usados para an\u00e1lise e relat\u00f3rios s\u00e3o tipicamente definidos e escritos em SQL ou atrav\u00e9s de interfaces de usu\u00e1rio de apontar e clicar. Assim como as considera\u00e7\u00f5es de construir versus comprar, h\u00e1 considera\u00e7\u00f5es ao escolher construir modelos usando SQL versus uma ferramenta no-code. SQL \u00e9 uma linguagem altamente acess\u00edvel que \u00e9 comum tanto para engenheiros de dados quanto para analistas. Ela capacita os analistas a trabalhar diretamente com os dados e otimizar o design dos modelos para suas necessidades. Tamb\u00e9m \u00e9 usada em quase todas as organiza\u00e7\u00f5es, proporcionando um ponto de entrada familiar para novos contratados em uma equipe. Na maioria dos casos, escolher um framework de transforma\u00e7\u00e3o que suporte a constru\u00e7\u00e3o de modelos de dados em SQL em vez de interfaces de usu\u00e1rio de apontar e clicar \u00e9 desej\u00e1vel. Voc\u00ea ter\u00e1 muito mais personaliza\u00e7\u00e3o e controle sobre seu processo de desenvolvimento de ponta a ponta.</p>"},{"location":"data_pipelines/pocket_reference/02-a-modern-data-infrastructure/#plataformas-de-orquestracao-de-workflows","title":"Plataformas de Orquestra\u00e7\u00e3o de Workflows","text":"<p>\u00c0 medida que a complexidade e o n\u00famero de pipelines de dados em uma organiza\u00e7\u00e3o crescem, \u00e9 importante introduzir uma plataforma de orquestra\u00e7\u00e3o de workflows na sua infraestrutura de dados. Essas plataformas gerenciam o agendamento e o fluxo de tarefas em uma pipeline. Imagine uma pipeline com uma d\u00fazia de tarefas que variam desde ingest\u00f5es de dados escritas em Python at\u00e9 transforma\u00e7\u00f5es de dados escritas em SQL que devem ser executadas em uma sequ\u00eancia espec\u00edfica e gerenciar depend\u00eancias entre cada tarefa. Toda equipe de dados enfrenta esse desafio, mas felizmente existem v\u00e1rias plataformas de orquestra\u00e7\u00e3o de workflows dispon\u00edveis para aliviar o problema.</p> <p>Algumas plataformas, como Apache Airflow, Luigi e AWS Glue, s\u00e3o projetadas para casos de uso mais gerais e, portanto, s\u00e3o usadas para uma ampla variedade de pipelines de dados. Outras, como Kubeflow Pipelines, s\u00e3o projetadas para casos de uso e plataformas mais espec\u00edficos (workflows de aprendizado de m\u00e1quina constru\u00eddos em cont\u00eaineres Docker no caso do Kubeflow Pipelines).</p>"},{"location":"data_pipelines/pocket_reference/02-a-modern-data-infrastructure/#grafos-aciclicos-dirigidos-directed-acyclic-graphs","title":"Grafos Ac\u00edclicos Dirigidos (Directed Acyclic Graphs)","text":"<p>Quase todas as plataformas modernas de orquestra\u00e7\u00e3o representam o fluxo e as depend\u00eancias das tarefas em um pipeline como um grafo. No entanto, os grafos de pipelines t\u00eam algumas restri\u00e7\u00f5es espec\u00edficas.</p> <p>Os grafos de pipelines tamb\u00e9m devem ser ac\u00edclicos, o que significa que uma tarefa n\u00e3o pode apontar de volta para uma tarefa previamente conclu\u00edda. Em outras palavras, n\u00e3o pode haver ciclos. Se pudesse, ent\u00e3o um pipeline poderia rodar indefinidamente!</p> <p>Com essas duas restri\u00e7\u00f5es em mente, os pipelines de orquestra\u00e7\u00e3o produzem grafos chamados grafos ac\u00edclicos dirigidos (DAGs).</p> <p>DAGs s\u00e3o uma representa\u00e7\u00e3o de um conjunto de tarefas e n\u00e3o onde a l\u00f3gica das tarefas \u00e9 definida. Uma plataforma de orquestra\u00e7\u00e3o \u00e9 capaz de executar tarefas de todos os tipos.</p> <p>A plataforma de orquestra\u00e7\u00e3o executa cada tarefa, mas a l\u00f3gica das tarefas existe como c\u00f3digo SQL e Python, que roda em diferentes sistemas na infraestrutura de dados.</p>"},{"location":"data_pipelines/pocket_reference/02-a-modern-data-infrastructure/#personalizando-sua-infraestrutura-de-dados","title":"Personalizando Sua Infraestrutura de Dados","text":"<p>\u00c9 raro encontrar duas organiza\u00e7\u00f5es com exatamente a mesma infraestrutura de dados. A maioria escolhe ferramentas e fornecedores que atendem \u00e0s suas necessidades espec\u00edficas e constr\u00f3i o restante por conta pr\u00f3pria.</p> <p>Como mencionado anteriormente, dependendo da cultura e dos recursos da sua organiza\u00e7\u00e3o, voc\u00ea pode ser incentivado a construir a maior parte da sua infraestrutura de dados por conta pr\u00f3pria ou a depender de fornecedores SaaS. Independentemente de qual caminho voc\u00ea escolha na escala de construir versus comprar, voc\u00ea pode construir a infraestrutura de dados de alta qualidade necess\u00e1ria para criar pipelines de dados de alta qualidade.</p> <p>O que \u00e9 importante \u00e9 entender suas restri\u00e7\u00f5es (dinheiro, recursos de engenharia, seguran\u00e7a e toler\u00e2ncia ao risco legal) e as compensa\u00e7\u00f5es resultantes.</p>"},{"location":"data_pipelines/pocket_reference/03-common-data-pipeline-patterns/","title":"Padr\u00f5es Comuns de Pipeline de Dados","text":"<p>Mesmo para engenheiros de dados experientes, projetar um novo pipeline de dados \u00e9 uma nova jornada a cada vez. Diferentes fontes de dados e infraestruturas apresentam tanto desafios quanto oportunidades. Al\u00e9m disso, os pipelines s\u00e3o constru\u00eddos com diferentes objetivos e restri\u00e7\u00f5es. Os dados precisam ser processados em quase tempo real? Podem ser atualizados diariamente? Ser\u00e3o modelados para uso em um dashboard ou como entrada para um modelo de aprendizado de m\u00e1quina?</p> <p>Felizmente, existem alguns padr\u00f5es comuns em pipelines de dados que se mostraram bem-sucedidos e s\u00e3o extens\u00edveis para muitos casos de uso.</p>"},{"location":"data_pipelines/pocket_reference/03-common-data-pipeline-patterns/#etl-e-elt","title":"ETL e ELT","text":"<p>Talvez n\u00e3o haja padr\u00e3o mais conhecido do que ETL e seu irm\u00e3o mais moderno, ELT. Ambos s\u00e3o padr\u00f5es amplamente utilizados em data warehousing e business intelligence. Nos \u00faltimos anos, eles inspiraram padr\u00f5es de pipeline para ci\u00eancia de dados e modelos de aprendizado de m\u00e1quina em produ\u00e7\u00e3o. Eles s\u00e3o t\u00e3o conhecidos que muitas pessoas usam esses termos como sin\u00f4nimos de pipelines de dados, em vez de padr\u00f5es que muitos pipelines seguem.</p> <p>Dadas suas ra\u00edzes em data warehousing, \u00e9 mais f\u00e1cil descrev\u00ea-los nesse contexto, que \u00e9 o que esta se\u00e7\u00e3o faz.</p> <p>Ambos os padr\u00f5es s\u00e3o abordagens de processamento de dados usadas para alimentar dados em um data warehouse e torn\u00e1-los \u00fateis para analistas e ferramentas de relat\u00f3rios. A diferen\u00e7a entre os dois \u00e9 a ordem de suas duas \u00faltimas etapas (transformar e carregar), mas as implica\u00e7\u00f5es de design ao escolher entre eles s\u00e3o substanciais.</p> <p>A etapa de extra\u00e7\u00e3o coleta dados de v\u00e1rias fontes em prepara\u00e7\u00e3o para carregamento e transforma\u00e7\u00e3o.</p> <p>A etapa de carregamento traz os dados brutos (no caso de ELT) ou os dados totalmente transformados (no caso de ETL) para o destino final. De qualquer forma, o resultado final \u00e9 carregar dados no data warehouse, data lake ou outro destino.</p> <p>A etapa de transforma\u00e7\u00e3o \u00e9 onde os dados brutos de cada sistema de origem s\u00e3o combinados e formatados de maneira que sejam \u00fateis para analistas, ferramentas de visualiza\u00e7\u00e3o ou qualquer caso de uso que seu pipeline esteja atendendo. H\u00e1 muito nesta etapa, independentemente de voc\u00ea projetar seu processo como ETL ou ELT.</p>"},{"location":"data_pipelines/pocket_reference/03-common-data-pipeline-patterns/#a-emergencia-do-elt-sobre-o-etl","title":"A Emerg\u00eancia do ELT sobre o ETL","text":"<p>ETL foi o padr\u00e3o ouro dos padr\u00f5es de pipeline de dados por d\u00e9cadas. Embora ainda seja usado, mais recentemente o ELT emergiu como o padr\u00e3o preferido. Por qu\u00ea? Antes da nova gera\u00e7\u00e3o de data warehouses, principalmente na nuvem, as equipes de dados n\u00e3o tinham acesso a data warehouses com a capacidade de armazenamento ou computa\u00e7\u00e3o necess\u00e1ria para lidar com o carregamento de grandes quantidades de dados brutos e transform\u00e1-los em modelos de dados utiliz\u00e1veis. Os bancos de dados eram baseados em linhas, funcionavam bem para casos de uso transacionais, mas n\u00e3o para consultas em grande volume, comuns em an\u00e1lises. Assim, os dados eram primeiro extra\u00eddos dos sistemas de origem e depois transformados em um sistema separado antes de serem carregados em um warehouse para qualquer modelagem final de dados e consulta por analistas e ferramentas de visualiza\u00e7\u00e3o.</p> <p>A maioria dos data warehouses de hoje s\u00e3o constru\u00eddos em bancos de dados colunar altamente escal\u00e1veis que podem armazenar e executar transforma\u00e7\u00f5es em grandes conjuntos de dados de maneira econ\u00f4mica. Gra\u00e7as \u00e0 efici\u00eancia de I/O de um banco de dados colunar, compress\u00e3o de dados e a capacidade de distribuir dados e consultas em muitos n\u00f3s que podem trabalhar juntos para processar dados, as coisas mudaram. Agora \u00e9 melhor focar em extrair dados e carreg\u00e1-los em um data warehouse onde voc\u00ea pode ent\u00e3o realizar as transforma\u00e7\u00f5es necess\u00e1rias para completar o pipeline.</p> <p>O impacto da diferen\u00e7a entre data warehouses baseados em linhas e colunas n\u00e3o pode ser subestimado.</p> <p>Abaixo, ilustra um exemplo de como os registros s\u00e3o armazenados em disco em um banco de dados baseado em linhas, como MySQL ou Postgres. Cada linha do banco de dados \u00e9 armazenada junta no disco, em um ou mais blocos dependendo do tamanho de cada registro. Se um registro for menor que um \u00fanico bloco ou n\u00e3o for divis\u00edvel pelo tamanho do bloco, ele deixa algum espa\u00e7o em disco n\u00e3o utilizado.</p> OrderId CustomerId ShippingCountry OrderTotal 1 1258 US 55.25 2 5698 AUS 125.36 3 2265 US 776.95 4 8954 CA 32.16 Bloco 1 1, 1258, US, 55.25 Bloco 2 2, 5698, AUS, 125.36 Bloco 3 3, 2265, US, 776.95 Bloco 4 4, 8954, CA, 32.16 <p>Uma tabela armazenada em um banco de dados de armazenamento baseado em linhas. Cada bloco cont\u00e9m um registro (linha) da tabela.</p> <p>Considere um caso de uso de banco de dados de processamento de transa\u00e7\u00f5es online (OLTP), como um aplicativo web de e-commerce que utiliza um banco de dados MySQL para armazenamento. O aplicativo web solicita leituras e grava\u00e7\u00f5es do banco de dados MySQL, frequentemente envolvendo m\u00faltiplos valores de cada registro, como os detalhes de um pedido em uma p\u00e1gina de confirma\u00e7\u00e3o de pedido. Tamb\u00e9m \u00e9 prov\u00e1vel que consulte ou atualize apenas um pedido por vez. Portanto, o armazenamento baseado em linhas \u00e9 ideal, pois os dados que o aplicativo precisa est\u00e3o armazenados em proximidade no disco, e a quantidade de dados consultada de uma vez \u00e9 pequena.</p> <p>O uso ineficiente do espa\u00e7o em disco devido aos registros deixarem espa\u00e7o vazio nos blocos \u00e9 uma troca razo\u00e1vel nesse caso, pois a velocidade de leitura e grava\u00e7\u00e3o de registros \u00fanicos frequentemente \u00e9 o mais importante. No entanto, em an\u00e1lises a situa\u00e7\u00e3o \u00e9 inversa. Em vez da necessidade de ler e escrever pequenas quantidades de dados frequentemente, frequentemente lemos e escrevemos grandes quantidades de dados com pouca frequ\u00eancia. Al\u00e9m disso, \u00e9 menos prov\u00e1vel que uma consulta anal\u00edtica exija muitas, ou todas, as colunas de uma tabela, mas sim uma \u00fanica coluna de uma tabela com muitas colunas.</p> <p>Por exemplo, considere a tabela de pedidos em nosso aplicativo de e-commerce funcional. Entre outras coisas, ela cont\u00e9m o valor em d\u00f3lares do pedido, bem como o pa\u00eds para o qual est\u00e1 sendo enviado. Ao contr\u00e1rio do aplicativo web, que trabalha com pedidos um de cada vez, um analista usando o data warehouse desejar\u00e1 analisar os dados do warehouse que possuem colunas adicionais que cont\u00eam valores de v\u00e1rias tabelas em nosso banco de dados MySQL. Por exemplo, pode conter informa\u00e7\u00f5es sobre o cliente que fez o pedido. Talvez o analista queira somar todos os pedidos feitos por clientes com contas ativas atualmente. Tal consulta pode envolver milh\u00f5es de registros, mas apenas ler de duas colunas, OrderTotal e CustomerActive. Afinal, a an\u00e1lise n\u00e3o \u00e9 sobre criar ou alterar dados (como no OLTP), mas sim a deriva\u00e7\u00e3o de m\u00e9tricas e a compreens\u00e3o dos dados.</p> <p>Abaixo, ilustra um banco de dados colunar, como Snowflake ou Amazon Redshift, que armazena dados em blocos de disco por coluna em vez de linha. No nosso caso, a consulta escrita pelo analista s\u00f3 precisa acessar blocos que armazenam valores de OrderTotal e CustomerActive em vez de blocos que armazenam os registros baseados em linhas, como no banco de dados MySQL. Assim, h\u00e1 menos I/O de disco, bem como menos dados para carregar na mem\u00f3ria para realizar o filtro e a soma exigidos pela consulta do analista. Um benef\u00edcio final \u00e9 a redu\u00e7\u00e3o no armazenamento, gra\u00e7as ao fato de que os blocos podem ser totalmente utilizados e comprimidos de forma otimizada, j\u00e1 que o mesmo tipo de dado \u00e9 armazenado em cada bloco em vez de m\u00faltiplos tipos que tendem a ocorrer em um \u00fanico registro baseado em linhas.</p> OrderId CustomerId ShippingCountry OrderTotal CustomerActive 1 1258 US 55.25 TRUE 2 5698 AUS 125.36 TRUE 3 2265 US 776.95 TRUE 4 8954 CA 32.16 FALSE Bloco 1 1, 2, 3, 4 Bloco 2 1258, 5698, 2265, 8954 Bloco 3 US, AUS, US, CA Bloco 4 55.25, 125.36, 776.95, 32.16 Bloco 5 TRUE, TRUE, TRUE, FALSE <p>Uma tabela armazenada em um banco de dados de armazenamento baseado em colunas. Cada bloco de disco cont\u00e9m dados da mesma coluna. As duas colunas envolvidas em nossa consulta de exemplo est\u00e3o destacadas. Apenas esses blocos precisam ser acessados para executar a consulta. Cada bloco cont\u00e9m dados do mesmo tipo, tornando a compress\u00e3o otimizada.</p> <p>No geral, a emerg\u00eancia dos bancos de dados colunar significa que armazenar, transformar e consultar grandes conjuntos de dados \u00e9 eficiente dentro de um data warehouse. Engenheiros de dados podem usar isso a seu favor construindo etapas de pipeline que se especializam em extrair e carregar dados em warehouses onde podem ser transformados, modelados e consultados por analistas e cientistas de dados que se sentem mais confort\u00e1veis dentro dos limites de um banco de dados. Como tal, o ELT assumiu como o padr\u00e3o ideal para pipelines de data warehouse, bem como outros casos de uso em aprendizado de m\u00e1quina e desenvolvimento de produtos de dados.</p>"},{"location":"data_pipelines/pocket_reference/03-common-data-pipeline-patterns/#subpadrao-etlt","title":"Subpadr\u00e3o EtLT","text":"<p>Quando o ELT emergiu como o padr\u00e3o dominante, ficou claro que fazer algumas transforma\u00e7\u00f5es ap\u00f3s a extra\u00e7\u00e3o, mas antes do carregamento, ainda era ben\u00e9fico. No entanto, em vez de transforma\u00e7\u00f5es envolvendo l\u00f3gica de neg\u00f3cios ou modelagem de dados, esse tipo de transforma\u00e7\u00e3o \u00e9 mais limitado em escopo.</p> <p>Alguns exemplos do tipo de transforma\u00e7\u00e3o que se encaixa no subpadr\u00e3o EtLT incluem os seguintes:</p> <ul> <li>Remover duplicatas em uma tabela</li> <li>Analisar par\u00e2metros de URL em componentes individuais</li> <li>Mascarar ou de outra forma ofuscar dados sens\u00edveis</li> </ul> <p>Esses tipos de transforma\u00e7\u00f5es est\u00e3o totalmente desconectados da l\u00f3gica de neg\u00f3cios ou, no caso de algo como mascarar dados sens\u00edveis, \u00e0s vezes s\u00e3o necess\u00e1rios o mais cedo poss\u00edvel em um pipeline por raz\u00f5es legais ou de seguran\u00e7a. Al\u00e9m disso, h\u00e1 valor em usar a ferramenta certa para o trabalho certo.</p> <p>Voc\u00ea pode assumir que os padr\u00f5es relacionados ao ELT restantes s\u00e3o projetados para incluir o subpadr\u00e3o EtLT tamb\u00e9m.</p>"},{"location":"data_pipelines/pocket_reference/03-common-data-pipeline-patterns/#elt-para-analise-de-dados","title":"ELT para An\u00e1lise de Dados","text":"<p>Como j\u00e1 discutido, bancos de dados colunar s\u00e3o bem adequados para lidar com grandes volumes de dados. Eles tamb\u00e9m s\u00e3o projetados para lidar com tabelas largas, ou seja, tabelas com muitas colunas, gra\u00e7as ao fato de que apenas os dados nas colunas usadas em uma determinada consulta s\u00e3o escaneados no disco e carregados na mem\u00f3ria.</p> <p>Al\u00e9m das considera\u00e7\u00f5es t\u00e9cnicas, os analistas de dados geralmente s\u00e3o fluentes em SQL. Com ELT, os engenheiros de dados podem se concentrar nas etapas de extra\u00e7\u00e3o e carregamento em um pipeline (ingest\u00e3o de dados), enquanto os analistas podem utilizar SQL para transformar os dados que foram ingeridos conforme necess\u00e1rio para relat\u00f3rios e an\u00e1lises. Tal separa\u00e7\u00e3o limpa n\u00e3o \u00e9 poss\u00edvel com o padr\u00e3o ETL, pois os engenheiros de dados s\u00e3o necess\u00e1rios em todo o pipeline.</p> <p>Al\u00e9m disso, o padr\u00e3o ELT reduz a necessidade de prever exatamente o que os analistas far\u00e3o com os dados no momento da constru\u00e7\u00e3o do processo de extra\u00e7\u00e3o e carregamento. Embora seja necess\u00e1rio entender o caso de uso geral para extrair e carregar os dados adequados, salvar a etapa de transforma\u00e7\u00e3o para mais tarde d\u00e1 aos analistas mais op\u00e7\u00f5es e flexibilidade.</p>"},{"location":"data_pipelines/pocket_reference/03-common-data-pipeline-patterns/#elt-para-ciencia-de-dados","title":"ELT para Ci\u00eancia de Dados","text":"<p>Pipelines de dados constru\u00eddos para equipes de ci\u00eancia de dados s\u00e3o semelhantes aos constru\u00eddos para an\u00e1lise de dados em um data warehouse. Assim como no caso de uso de an\u00e1lise, os engenheiros de dados est\u00e3o focados em ingerir dados em um data warehouse ou data lake. No entanto, os cientistas de dados t\u00eam necessidades diferentes dos analistas de dados.</p> <p>Embora a ci\u00eancia de dados seja um campo amplo, em geral, os cientistas de dados precisar\u00e3o de acesso a dados mais granulares e, \u00e0s vezes, brutos do que os analistas de dados. Enquanto os analistas de dados constroem modelos de dados que produzem m\u00e9tricas e alimentam dashboards, os cientistas de dados passam seus dias explorando dados e construindo modelos preditivos.</p> <p>Se voc\u00ea est\u00e1 construindo pipelines para apoiar cientistas de dados, descobrir\u00e1 que as etapas de extra\u00e7\u00e3o e carregamento do padr\u00e3o ELT permanecer\u00e3o praticamente as mesmas que para suportar an\u00e1lises.</p>"},{"location":"data_pipelines/pocket_reference/03-common-data-pipeline-patterns/#elt-para-produtos-de-dados-e-aprendizado-de-maquina","title":"ELT para Produtos de Dados e Aprendizado de M\u00e1quina","text":"<p>Os dados s\u00e3o usados para mais do que an\u00e1lise, relat\u00f3rios e modelos preditivos. Eles tamb\u00e9m s\u00e3o usados para alimentar produtos de dados. Alguns exemplos comuns de produtos de dados incluem os seguintes:</p> <ul> <li>Um mecanismo de recomenda\u00e7\u00e3o de conte\u00fado que alimenta a tela inicial de um servi\u00e7o de streaming de v\u00eddeo</li> <li>Um mecanismo de busca personalizado em um site de e-commerce</li> <li>Um aplicativo que realiza an\u00e1lise de sentimento em avalia\u00e7\u00f5es de restaurantes geradas por usu\u00e1rios</li> </ul> <p>Cada um desses produtos de dados \u00e9 provavelmente alimentado por um ou mais modelos de aprendizado de m\u00e1quina (ML), que s\u00e3o \u00e1vidos por dados de treinamento e valida\u00e7\u00e3o. Esses dados podem vir de uma variedade de sistemas de origem e passar por algum n\u00edvel de transforma\u00e7\u00e3o para prepar\u00e1-los para uso no modelo. Um padr\u00e3o semelhante ao ELT \u00e9 bem adequado para tais necessidades, embora existam v\u00e1rios desafios espec\u00edficos em todas as etapas de um pipeline projetado para um produto de dados.</p>"},{"location":"data_pipelines/pocket_reference/03-common-data-pipeline-patterns/#etapas-em-um-pipeline-de-aprendizado-de-maquina","title":"Etapas em um Pipeline de Aprendizado de M\u00e1quina","text":"<p>Assim como os pipelines constru\u00eddos para an\u00e1lise, os pipelines constru\u00eddos para ML seguem um padr\u00e3o semelhante ao ELT -- pelo menos no in\u00edcio do pipeline. A diferen\u00e7a \u00e9 que, em vez da etapa de transforma\u00e7\u00e3o focar em transformar dados em modelos de dados, uma vez que os dados s\u00e3o extra\u00eddos e carregados em um warehouse ou data lake, h\u00e1 v\u00e1rias etapas envolvidas na constru\u00e7\u00e3o e atualiza\u00e7\u00e3o do modelo de ML.</p> <p>Se voc\u00ea est\u00e1 familiarizado com o desenvolvimento de ML, essas etapas podem parecer familiares  amb\u00e9m:</p> <p>Ingest\u00e3o de dados Embora os dados que voc\u00ea ingere possam diferir, a l\u00f3gica permanece basicamente a mesma para pipelines constru\u00eddos para an\u00e1lise, bem como para ML, mas com uma considera\u00e7\u00e3o adicional para pipelines de ML. Ou seja, garantir que os dados que voc\u00ea ingere sejam versionados de uma forma que os modelos de ML possam posteriormente se referir como um conjunto de dados espec\u00edfico para treinamento ou valida\u00e7\u00e3o.</p> <p>Pr\u00e9-processamento de dados Os dados ingeridos provavelmente n\u00e3o estar\u00e3o prontos para uso no desenvolvimento de ML. O pr\u00e9-processamento \u00e9 onde os dados s\u00e3o limpos e preparados para os modelos. Por exemplo, esta \u00e9 a etapa em um pipeline onde o texto \u00e9 tokenizado, os recursos s\u00e3o convertidos em valores num\u00e9ricos e os valores de entrada s\u00e3o normalizados.</p> <p>Treinamento de modelo Ap\u00f3s novos dados serem ingeridos e pr\u00e9-processados, os modelos de ML precisam ser re-treinados.</p> <p>Implementa\u00e7\u00e3o de modelo Implantar modelos em produ\u00e7\u00e3o pode ser a parte mais desafiadora de passar de aprendizado de m\u00e1quina orientado a pesquisa para um verdadeiro produto de dados. Aqui, n\u00e3o apenas a versionamento de conjuntos de dados \u00e9 necess\u00e1rio, mas tamb\u00e9m a versionamento de modelos treinados. Frequentemente, uma API REST \u00e9 usada para permitir a consulta de um modelo implantado, e endpoints de API para v\u00e1rias vers\u00f5es de um modelo ser\u00e3o usados. \u00c9 muito para acompanhar e requer coordena\u00e7\u00e3o entre cientistas de dados, engenheiros de aprendizado de m\u00e1quina e engenheiros de dados para chegar a um estado de produ\u00e7\u00e3o. Um pipeline bem projetado \u00e9 fundamental para unir tudo isso.</p>"},{"location":"data_pipelines/pocket_reference/03-common-data-pipeline-patterns/#incorporar-feedback-no-pipeline","title":"Incorporar Feedback no Pipeline","text":"<p>Qualquer bom pipeline de ML tamb\u00e9m incluir\u00e1 a coleta de feedback para melhorar o modelo. Tome como exemplo um modelo de recomenda\u00e7\u00e3o de conte\u00fado para um servi\u00e7o de streaming de v\u00eddeo. Para medir e melhorar o modelo no futuro, voc\u00ea precisar\u00e1 acompanhar o que ele recomenda aos usu\u00e1rios, quais recomenda\u00e7\u00f5es eles clicam e qual conte\u00fado recomendado eles gostam ap\u00f3s clicarem. Para fazer isso, voc\u00ea precisar\u00e1 trabalhar com a equipe de desenvolvimento que utiliza o modelo na tela inicial do servi\u00e7o de streaming. Eles precisar\u00e3o implementar algum tipo de coleta de eventos que acompanhe cada recomenda\u00e7\u00e3o feita a cada usu\u00e1rio; a vers\u00e3o do modelo que a recomendou; e quando \u00e9 clicada; e ent\u00e3o levar esse clique para os dados que provavelmente j\u00e1 est\u00e3o coletando relacionados ao consumo de conte\u00fado do usu\u00e1rio.</p> <p>Todas essas informa\u00e7\u00f5es podem ent\u00e3o ser ingeridas de volta no data warehouse e incorporadas em futuras vers\u00f5es do modelo, seja como dados de treinamento ou para serem analisadas e consideradas por um humano (talvez um cientista de dados) para inclus\u00e3o em um futuro modelo ou experimento.</p> <p>Os analistas frequentemente ser\u00e3o encarregados de medir a efic\u00e1cia dos modelos e construir dashboards para exibir m\u00e9tricas chave do modelo para a organiza\u00e7\u00e3o. Os stakeholders podem usar esses dashboards para entender qu\u00e3o eficazes v\u00e1rios modelos s\u00e3o para o neg\u00f3cio e para seus clientes.</p>"}]}